{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+Wc8bosiflkZ4F/k1hwXO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoTorella/progettoLabAI/blob/main/training1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR61Ia6xtuiS",
        "outputId": "1701c35d-69dd-45fa-9686-95f363738f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.2.2)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.25.2)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.10 snuggs-1.4.7\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "import rasterio\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "XMKGMtrSt_uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_lEkueauCnK",
        "outputId": "35cc5b5c-21ec-48bc-e530-24084b7ebcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHMUgllbdP26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9EH85vNMATp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r drive/MyDrive/progettoLabAi3/train/mask/val_set /content/mask"
      ],
      "metadata": {
        "id": "k42HDyasuNk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QzjK4f8vaFW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultispectralDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir   #directory delle immagini di train senza label\n",
        "        self.mask_dir = mask_dir    #directory delle label\n",
        "        self.transform = transform  #trasformazione\n",
        "        self.images = os.listdir(image_dir) #lista di stringhe contenente tutti i nomi delle immagini senza label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)    #lunghezza della stringa che contiene tutti i path, quindi grandezza del dataset di train\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index]) #ricostruisce il persorso completo dell'immagine corrispondente all'indice fornito\n",
        "        #mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".tif\", \"_mask.tiff\").replace(\"PS-RGBNIR\",\"Buildings\")) #ricostruisce il percorso completo della label corrispondente all'indice fornito, inoltre sostituisce .jpg con .gif, qui si assume che il nome della label sia uguale a quello dell'immagine con l'estensione eccetto, devo modificare questa riga in seguito\n",
        "        string = self.images[index]\n",
        "        mask_name = string.replace('PS-RGBNIR','Buildings').replace(\".tif\",\"_mask.tiff\")\n",
        "        #print(\"STRINGA\"+ string)\n",
        "        #tile_index= indice_numero = max(string.rfind('_'), string.rfind(' '))\n",
        "        #mask_name = string[:tile_index + 1] + \"mask_\" + string[tile_index + 1:]\n",
        "        #print(\"FINALE: \"+ mask_name)\n",
        "        #mask_name = mask_name.replace(\"11 mask_2.tiff\",\"mask_11.tiff\")\n",
        "        #mask_name = mask_name.replace(\"12 mask_2.tiff\",\"mask_12.tiff\")\n",
        "        #mask_name = mask_name.replace(\"13 mask_2.tiff\",\"mask_23.tiff\")\n",
        "        #mask_name = mask_name.replace(\"21 mask_2.tiff\",\"mask_21.tiff\")\n",
        "        #mask_name = mask_name.replace(\"22 mask_2.tiff\",\"mask_22.tiff\")\n",
        "        #mask_name = mask_name.replace(\"23 mask_2.tiff\",\"mask_23.tiff\")\n",
        "        #mask_name = mask_name.replace(\"31 mask_2.tiff\",\"mask_31.tiff\")\n",
        "        #mask_name = mask_name.replace(\"32 mask_2.tiff\",\"mask_32.tiff\")\n",
        "        #mask_name = mask_name.replace(\"33 mask_2.tiff\",\"mask_33.tiff\")\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name) #ricostruisce il percorso completo della label corrispondente all'indice fornito, inoltre sostituisce .jpg con .gif, qui si assume che il nome della label sia uguale a quello dell'immagine con l'estensione eccetto, devo modificare questa riga in seguito\n",
        "        with rasterio.open(img_path) as src:\n",
        "            image = src.read().transpose((1, 2, 0)).astype(np.float32) #trasforma l'immagine da formato (C, H, W) a (H, W, C)\n",
        "        #with rasterio.open(mask_path) as src:\n",
        "            #mask = src.read().transpose((1,2,0)).astype(np.float32)\n",
        "        with rasterio.open(mask_path) as src:\n",
        "          mask = src.read().astype(np.float32)\n",
        "        #mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.long)\n",
        "\n",
        "\n",
        "        #print(\"ciao\")\n",
        "        #plt.imshow(mask)\n",
        "        #plt.title(f\"Mask 1:\")\n",
        "        #plt.axis('off')\n",
        "        #plt.show()\n",
        "        #apre l'immagine e la converte in scala di grigi, inoltre la trasforma in un array numpy di float32\n",
        "        mask[mask == 255.0] = 1.0 #sostituisce tutti i valori 255 con 1, serve per la normalizzazione\n",
        "        mask = mask.squeeze(0)\n",
        "        #plt.imshow(mask)\n",
        "        #plt.title(f\"Mask 2:\")\n",
        "        #plt.axis('off')\n",
        "        #plt.show()\n",
        "\n",
        "        if self.transform is not None: #se è stata definita una trasformazione\n",
        "            augmentations = self.transform(image=image, mask=mask) #applica le trasformazioni all'immagine e alla label, restituisce un dizionario con chiavi image e mask\n",
        "            image = augmentations[\"image\"] #estrai dal dizionario l'immagine e la assegna alla variabile image\n",
        "            mask = augmentations[\"mask\"] #estrai dal dizionario la label e la assegna alla variabile mask\n",
        "\n",
        "        #plt.imshow(mask)\n",
        "        #plt.title(f\"Mask 3:\")\n",
        "        #plt.axis('off')\n",
        "        #plt.show()\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "LooUjU6eubOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "tYTWqxW7ub7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#salva il checkpoint del modello in un file .pth.tar, state è un dizionario che contiene lo stato del modello e dell'ottimizzatore, filename è il nome del file in cui salvare il checkpoint\n",
        "def save_checkpoint(state, filename=\"/content/drive/My Drive/progettoLabAi3/computations/saved/my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "#carica il checkpoint del modello da un file .pth.tar, checkpoint è un dizionario che contiene lo stato del modello e dell'ottimizzatore, model è l'istanza del modello, checkpoint è un dizionario che contiene lo stato del modello e dell'ottimizzatore salvato in un file .pth.tar, model è l'istanza del modello in cui caricare il checkpoint\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "\n",
        "    #show_random_image_from_directory(train_maskdir)\n",
        "\n",
        "    #crea un'istanza della classe MultispectralDataset, che rappresenta il dataset di addestramento, image_dir è la directory delle immagini di addestramento, mask_dir è la directory delle label di addestramento, transform è la trasformazione da applicare al dataset\n",
        "    train_ds = MultispectralDataset(\n",
        "        image_dir=train_dir,\n",
        "        mask_dir=train_maskdir,\n",
        "        transform=train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers, #numero di processi in background da utilizzare per il caricamento dei dati\n",
        "        pin_memory=pin_memory, #se True, i dati verranno copiati in memoria GPU prima di essere passati al modello, nel mio caso è inutile settarlo a True perché non ho una GPU dedicata\n",
        "        shuffle=True, #se True, i dati verranno mescolati prima di essere passati al modello in ogni epoca di addestramento\n",
        "    )\n",
        "\n",
        "    #crea un'istanza della classe MultispectralDataset, che rappresenta il dataset di validazione, image_dir è la directory delle immagini di validazione, mask_dir è la directory delle label di validazione, transform è la trasformazione da applicare al dataset\n",
        "    val_ds = MultispectralDataset(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "\n",
        "    #stessa cosa di train_loader ma per il dataset di validazione, infatti passiamo val_ds come primo argomento, però shuffle=False perché non serve mescolare i dati di validazione\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def check_accuracy(loader, model, loss_fn, file, limited=False,limit=32,device=\"cuda\"):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    f1 = 0\n",
        "    val_loss = 0\n",
        "    iteration = 1\n",
        "    print(f'interations: {len(loader)}')\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            print(f'Iteration number: {iteration}')\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            if limited:\n",
        "              if(iteration > limit):\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "\n",
        "            out = model(x)\n",
        "            val_loss += loss_fn(out, y).item()\n",
        "            preds = torch.sigmoid(out)\n",
        "            preds = (preds > 0.5)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8) #preds or y\n",
        "            precision += precision_score(y.cpu().numpy().flatten(), preds.cpu().numpy().flatten(), zero_division=1)\n",
        "            recall += recall_score(y.cpu().numpy().flatten(), preds.cpu().numpy().flatten(), zero_division=1)\n",
        "            f1 += f1_score(y.cpu().numpy().flatten(), preds.cpu().numpy().flatten(), zero_division=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if limited:\n",
        "      denominator = min(limit, len(loader))\n",
        "    else:\n",
        "      denominator = len(loader)\n",
        "\n",
        "    val_loss /= denominator\n",
        "    accuracy = num_correct / num_pixels * 100\n",
        "    dice = dice_score / denominator\n",
        "    precision_avg = precision / denominator\n",
        "    recall_avg = recall / denominator\n",
        "    f1_avg = f1 / denominator\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Dice score: {dice}\")\n",
        "    print(f\"Precision: {precision_avg}\")\n",
        "    print(f\"Recall: {recall_avg}\")\n",
        "    print(f\"F1 Score: {f1_avg}\")\n",
        "\n",
        "    # Salva i risultati su un file di testo\n",
        "    with open(file, \"a\") as f:\n",
        "        f.write(f\"Validation Loss: {val_loss:.4f}\\n\")\n",
        "        f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
        "        f.write(f\"Dice score: {dice}\\n\")\n",
        "        f.write(f\"Precision: {precision_avg}\\n\")\n",
        "        f.write(f\"Recall: {recall_avg}\\n\")\n",
        "        f.write(f\"F1 Score: {f1_avg}\\n\")\n",
        "        f.write(\"\\n\")  # Riga vuota per separare le epoche\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder=\"/content/drive/My Drive/progettoLabAi3/computations/saved/images/\",\n",
        "                             device=\"cuda\"):\n",
        "    model.eval()  # Imposta il modello in modalità di valutazione\n",
        "    print(f\"Saving images to {folder}\")\n",
        "    iteration = 1\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        print(\"Iteration: \", iteration)\n",
        "        iteration += 1\n",
        "        x = x.to(device=device)\n",
        "        y = y.to(device=device).unsqueeze(1)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "        # Concatenare le immagini (maschera originale e predizione) lungo l'asse verticale\n",
        "        combined = torch.cat((y, preds), dim=2)\n",
        "\n",
        "        # Salvare l'immagine combinata\n",
        "        torchvision.utils.save_image(combined, f\"{folder}/comparison_{idx}.png\")\n",
        "\n",
        "    model.train()  # Ripristina il modello in modalità di addestramento"
      ],
      "metadata": {
        "id": "K3MpksKKucPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_image_mask(image, mask, index):\n",
        "    # Utilizza solo i primi tre canali (R, G, B)\n",
        "    image_rgb = image[:, :, :3]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(image_rgb)\n",
        "    axs[0].set_title(f\"Image {index}\")\n",
        "    axs[1].imshow(mask, cmap='gray')\n",
        "    axs[1].set_title(f\"Mask {index}\")\n",
        "    plt.show()\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def write_description(description, file):\n",
        "  with open(file, \"a\") as f:\n",
        "    f.write(f\"Descrizione: {description}\\n\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "neAhm3ikugnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iUm7Yts_ugUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters etc.\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 20\n",
        "NUM_WORKERS = 4\n",
        "IMAGE_HEIGHT = 300\n",
        "IMAGE_WIDTH = 300\n",
        "STEP_SIZE = 18\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "TRAIN_IMG_DIR = \"/content/images/train_set\"\n",
        "TRAIN_MASK_DIR = \"/content/mask/train_set\"\n",
        "VAL_IMG_DIR = \"/content/images/val_set\"\n",
        "VAL_MASK_DIR = \"/content/mask/val_set\"\n",
        "MODEL_DIRECTORY = \"/content/drive/My Drive/progettoLabAi3/computations/saved/model15/\"\n",
        "MODEL_DESCRIPTION = \"modello identico al primo ma con la media e varianza calcolate da me\"\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler,file):\n",
        "    loop = tqdm(loader)  # Aggiunge una barra di avanzamento\n",
        "\n",
        "\n",
        "\n",
        "    # Enumerate restituisce una tupla contenente un contatore e il valore restituito dall'iterabile\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "\n",
        "        #print(f'data.shape: {data.shape}, data.dtype: {data.dtype}')\n",
        "        #print(f'targets.shape: {targets.shape}, targets.dtype: {targets.dtype}')\n",
        "        data = data.to(device=DEVICE)  # Trasferisce i dati sulla GPU\n",
        "        targets = targets.float().to(device=DEVICE)  # Trasferisce le label sulla GPU e aggiunge una dimensione\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data).squeeze(1) #predictions è un tensore di predizioni, ad esempio (16, 1, 300, 300)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() #azzera i gradienti\n",
        "        scaler.scale(loss).backward() #calcola i gradienti e scala i gradienti\n",
        "        scaler.step(optimizer) #aggiorna i pesi\n",
        "        scaler.update() #descala i pesi\n",
        "\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        last_loss = loss.item()  # Memorizza l'ultima loss calcolata\n",
        "\n",
        "    # Salva l'ultima loss calcolata su un file di testo\n",
        "    with open(file, \"a\") as f:\n",
        "        f.write(f\"Train Loss: {last_loss:.4f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    set_seed(42)\n",
        "\n",
        "    print(\"Creating transforms\")\n",
        "    train_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            #A.CenterCrop(height=896,width=896),\n",
        "            #A.RandomCrop(height=320,width=320),\n",
        "            #A.Rotate(limit=90, p=0.5),\n",
        "            #A.HorizontalFlip(p=0.5),\n",
        "            #A.VerticalFlip(p=0.5),\n",
        "            A.Normalize(\n",
        "                #mean=[84.882515, 114.9725, 122.400505, 264.03476],\n",
        "                #std=[112.35377, 132.05618, 154.0384, 294.54465],\n",
        "                mean=[81.2788, 111.04898, 117.07683, 261.56107],\n",
        "                std=[110.35951, 129.69858 ,151.0430,296.55923],\n",
        "                #max_pixel_value=65535.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "    )\n",
        "    val_transforms = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            #A.CenterCrop(height=896,width=896),\n",
        "            #A.RandomCrop(height=320,width=320),\n",
        "            A.Normalize(\n",
        "                #mean=[81.27881, 111.04898, 117.07683, 261.56107],\n",
        "                #std=[110.35951, 129.69858, 151.043, 296.55923],\n",
        "                mean=[84.882515, 114.9725, 122.400505, 264.03476],\n",
        "                std=[112.35377, 132.05618, 154.0384, 294.54465],\n",
        "                #max_pixel_value=65535.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    write_description(MODEL_DESCRIPTION, file= MODEL_DIRECTORY +\"outputs/outputs.txt\")\n",
        "\n",
        "    print(\"Creating model\")\n",
        "    model = UNET(in_channels=4, out_channels=1).to(DEVICE)\n",
        "    print(\"Model created\")\n",
        "    print(\"creating loss function and optimizer\")\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE)\n",
        "    print(\"loss function and optimizer created\")\n",
        "\n",
        "    print(\"Creating data loaders\")\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        TRAIN_IMG_DIR,\n",
        "        TRAIN_MASK_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        VAL_MASK_DIR,\n",
        "        BATCH_SIZE,\n",
        "        train_transform,\n",
        "        val_transforms,\n",
        "        NUM_WORKERS,\n",
        "        PIN_MEMORY,\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"data loaders created\")\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(MODEL_DIRECTORY +\"my_checkpoint.pth.tar\"), model)\n",
        "\n",
        "    print(\"checking accuracy on the validation set before training\")\n",
        "    #check_accuracy(\n",
        "     #       val_loader,\n",
        "     #       model,\n",
        "     #       loss_fn,\n",
        "     #       file= MODEL_DIRECTORY +\"outputs/outputs.txt\",\n",
        "     #       limited = True,\n",
        "     #       device=DEVICE\n",
        "    #)\n",
        "    #save_predictions_as_imgs(\n",
        "     #         val_loader, model, folder=MODEL_DIRECTORY +\"images/\", device=DEVICE\n",
        "    #)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    print(\"Starting training\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"epoch {epoch}\")\n",
        "        print(\"starting training\")\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler, file= MODEL_DIRECTORY +\"outputs/outputs.txt\")\n",
        "        print(\"training done\")\n",
        "\n",
        "        print(\"saving model\")\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint,filename = MODEL_DIRECTORY + \"my_checkpoint.pth.tar\")\n",
        "        print(\"model saved\")\n",
        "\n",
        "        print(\"checking accuracy on the validation set after training\")\n",
        "        check_accuracy(\n",
        "            val_loader,\n",
        "            model,\n",
        "            loss_fn,\n",
        "            file= MODEL_DIRECTORY +\"outputs/outputs.txt\",\n",
        "            limited = False,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        if((epoch) % 2 == 0):\n",
        "          print(\"saving predictions to folder\")\n",
        "          save_predictions_as_imgs(\n",
        "              val_loader, model, folder=MODEL_DIRECTORY +\"images/\", device=DEVICE\n",
        "          )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pphpLTpVucqm",
        "outputId": "5aa027f6-c307-43d3-e697-9de5a3d67965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating transforms\n",
            "Creating model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created\n",
            "creating loss function and optimizer\n",
            "loss function and optimizer created\n",
            "Creating data loaders\n",
            "data loaders created\n",
            "checking accuracy on the validation set before training\n",
            "Starting training\n",
            "epoch 0\n",
            "starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/149 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "100%|██████████| 149/149 [02:10<00:00,  1.14it/s, loss=0.302]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training done\n",
            "saving model\n",
            "=> Saving checkpoint\n",
            "model saved\n",
            "checking accuracy on the validation set after training\n",
            "interations: 32\n",
            "Iteration number: 1\n",
            "Iteration number: 2\n",
            "Iteration number: 3\n",
            "Iteration number: 4\n",
            "Iteration number: 5\n",
            "Iteration number: 6\n",
            "Iteration number: 7\n",
            "Iteration number: 8\n",
            "Iteration number: 9\n",
            "Iteration number: 10\n",
            "Iteration number: 11\n",
            "Iteration number: 12\n",
            "Iteration number: 13\n",
            "Iteration number: 14\n",
            "Iteration number: 15\n",
            "Iteration number: 16\n",
            "Iteration number: 17\n",
            "Iteration number: 18\n",
            "Iteration number: 19\n",
            "Iteration number: 20\n",
            "Iteration number: 21\n",
            "Iteration number: 22\n",
            "Iteration number: 23\n",
            "Iteration number: 24\n",
            "Iteration number: 25\n",
            "Iteration number: 26\n",
            "Iteration number: 27\n",
            "Iteration number: 28\n",
            "Iteration number: 29\n",
            "Iteration number: 30\n",
            "Iteration number: 31\n",
            "Iteration number: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2999\n",
            "Accuracy: 92.58\n",
            "Dice score: 0.0015259786741808057\n",
            "Precision: 0.06031717205377701\n",
            "Recall: 0.0007873315444749444\n",
            "F1 Score: 0.001525978632055621\n",
            "saving predictions to folder\n",
            "Saving images to /content/drive/My Drive/progettoLabAi3/computations/saved/model15/images/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "epoch 1\n",
            "starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [02:02<00:00,  1.21it/s, loss=0.196]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training done\n",
            "saving model\n",
            "=> Saving checkpoint\n",
            "model saved\n",
            "checking accuracy on the validation set after training\n",
            "interations: 32\n",
            "Iteration number: 1\n",
            "Iteration number: 2\n",
            "Iteration number: 3\n",
            "Iteration number: 4\n",
            "Iteration number: 5\n",
            "Iteration number: 6\n",
            "Iteration number: 7\n",
            "Iteration number: 8\n",
            "Iteration number: 9\n",
            "Iteration number: 10\n",
            "Iteration number: 11\n",
            "Iteration number: 12\n",
            "Iteration number: 13\n",
            "Iteration number: 14\n",
            "Iteration number: 15\n",
            "Iteration number: 16\n",
            "Iteration number: 17\n",
            "Iteration number: 18\n",
            "Iteration number: 19\n",
            "Iteration number: 20\n",
            "Iteration number: 21\n",
            "Iteration number: 22\n",
            "Iteration number: 23\n",
            "Iteration number: 24\n",
            "Iteration number: 25\n",
            "Iteration number: 26\n",
            "Iteration number: 27\n",
            "Iteration number: 28\n",
            "Iteration number: 29\n",
            "Iteration number: 30\n",
            "Iteration number: 31\n",
            "Iteration number: 32\n",
            "Validation Loss: 0.2661\n",
            "Accuracy: 92.74\n",
            "Dice score: 0.0\n",
            "Precision: 1.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n",
            "epoch 2\n",
            "starting training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/149 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  8%|▊         | 12/149 [00:12<02:21,  1.03s/it, loss=0.202]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-435492d9bffd>\u001b[0m in \u001b[0;36m<cell line: 177>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-435492d9bffd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMODEL_DIRECTORY\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"outputs/outputs.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-435492d9bffd>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, file)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#azzera i gradienti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#calcola i gradienti e scala i gradienti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#aggiorna i pesi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#descala i pesi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    349\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    348\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    349\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}